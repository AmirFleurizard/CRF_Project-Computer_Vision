{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VP0Qf6Um1mQq"},"outputs":[],"source":["import math\n","\n","from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format = 'retina'\n","\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","import torch\n","from torch import nn\n","from torchvision.models import resnet50\n","import torchvision.transforms as T\n","torch.set_grad_enabled(False);\n","\n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import os\n","import scipy.io\n","import shutil\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","source":["Implementation using Vision Transformer\n","Import data"],"metadata":{"id":"tDCQyNrQ1okP"}},{"cell_type":"code","source":["pip install -U tensorflow-addons"],"metadata":{"id":"7j_HtUkx-4B2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668880446121,"user_tz":300,"elapsed":4866,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"475e435e-fe10-4c31-d55b-4b7c3f840c24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 34.1 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.18.0\n"]}]},{"cell_type":"code","source":["# Path to images and annotations\n","path_images = \"/content/gdrive/My Drive/CV PROJ/ViT Implementation/yolo-dataset/train/images\"\n","path_annot = \"/content/gdrive/My Drive/CV PROJ/ViT Implementation/yolo-dataset/train/labels\"\n","\n","\n","# list of paths to images and annotations\n","image_paths = [\n","    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n","]\n","annot_paths = [\n","    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n","]\n","\n","image_paths.sort()\n","annot_paths.sort()\n","\n","image_size = 224  # resize input images to this size\n","\n","images, targets = [], []\n","\n","# loop over the annotations and images, preprocess them and store in lists\n","for i in range(0, len(annot_paths)):\n","    # Access bounding box coordinates\n","    annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n","\n","    top_left_x, top_left_y = annot[2], annot[0]\n","    bottom_right_x, bottom_right_y = annot[3], annot[1]\n","\n","    image = keras.utils.load_img(\n","        path_images + image_paths[i],\n","    )\n","    (w, h) = image.size[:2]\n","\n","    # resize train set images\n","    if i < int(len(annot_paths) * 0.8):\n","        # resize image if it is for training dataset\n","        image = image.resize((image_size, image_size))\n","\n","    # convert image to array and append to list\n","    images.append(keras.utils.img_to_array(image))\n","\n","    # apply relative scaling to bounding boxes as per given image and append to list\n","    targets.append(\n","        (\n","            float(top_left_x) / w,\n","            float(top_left_y) / h,\n","            float(bottom_right_x) / w,\n","            float(bottom_right_y) / h,\n","        )\n","    )\n","\n","# Convert the list to numpy array, split to train and test dataset\n","(x_train), (y_train) = (\n","    np.asarray(images[: int(len(images) * 0.8)]),\n","    np.asarray(targets[: int(len(targets) * 0.8)]),\n",")\n","(x_test), (y_test) = (\n","    np.asarray(images[int(len(images) * 0.8) :]),\n","    np.asarray(targets[int(len(targets) * 0.8) :]),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"ialHtwtvB0wI","executionInfo":{"status":"error","timestamp":1668880631925,"user_tz":300,"elapsed":145,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"146e35a9-e7d1-4cbd-ec07-ff5acf4f3854"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1d6137330487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# list of paths to images and annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m image_paths = [\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     10\u001b[0m annot_paths = [\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/CV PROJ/ViT Implementation/yolo-dataset/train/images'"]}]},{"cell_type":"code","source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x"],"metadata":{"id":"q5RQL7feCkq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    #     Override function to avoid error while saving model\n","    def get_config(self):\n","        config = super().get_config().copy()\n","        config.update(\n","            {\n","                \"input_shape\": input_shape,\n","                \"patch_size\": patch_size,\n","                \"num_patches\": num_patches,\n","                \"projection_dim\": projection_dim,\n","                \"num_heads\": num_heads,\n","                \"transformer_units\": transformer_units,\n","                \"transformer_layers\": transformer_layers,\n","                \"mlp_head_units\": mlp_head_units,\n","            }\n","        )\n","        return config\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        # return patches\n","        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"8hsRaZSwCzbv","executionInfo":{"status":"error","timestamp":1668880848249,"user_tz":300,"elapsed":138,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"4357e7e2-2238-42c1-add5-26759c3b015f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-e30fda085a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"]}]},{"cell_type":"code","source":["patch_size = 32  # Size of the patches to be extracted from the input images\n","\n","plt.figure(figsize=(4, 4))\n","plt.imshow(x_train[0].astype(\"uint8\"))\n","plt.axis(\"off\")\n","\n","patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n","print(f\"Image size: {image_size} X {image_size}\")\n","print(f\"Patch size: {patch_size} X {patch_size}\")\n","print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n","\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(4, 4))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"Pe7WqWCcC05j","executionInfo":{"status":"error","timestamp":1668880852700,"user_tz":300,"elapsed":138,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"d32f979f-e140-4e21-e49e-3a772f3266c1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-7976f1f65e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    # Override function to avoid error while saving model\n","    def get_config(self):\n","        config = super().get_config().copy()\n","        config.update(\n","            {\n","                \"input_shape\": input_shape,\n","                \"patch_size\": patch_size,\n","                \"num_patches\": num_patches,\n","                \"projection_dim\": projection_dim,\n","                \"num_heads\": num_heads,\n","                \"transformer_units\": transformer_units,\n","                \"transformer_layers\": transformer_layers,\n","                \"mlp_head_units\": mlp_head_units,\n","            }\n","        )\n","        return config\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"gAY78fDwC0r6","executionInfo":{"status":"error","timestamp":1668880856240,"user_tz":300,"elapsed":233,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"0aa7cae1-21d8-4a07-94b8-c08721594482"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ae3023b89ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPatchEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatchEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"]}]},{"cell_type":"code","source":["#VIT\n","def create_vit_object_detector(\n","    input_shape,\n","    patch_size,\n","    num_patches,\n","    projection_dim,\n","    num_heads,\n","    transformer_units,\n","    transformer_layers,\n","    mlp_head_units,\n","):\n","    inputs = layers.Input(shape=input_shape)\n","    # Create patches\n","    patches = Patches(patch_size)(inputs)\n","    # Encode patches\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.3)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n","\n","    bounding_box = layers.Dense(4)(\n","        features\n","    )  # Final four neurons that output bounding box\n","\n","    # return Keras model.\n","    return keras.Model(inputs=inputs, outputs=bounding_box)\n"],"metadata":{"id":"06KkMxl5C84p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n","\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","\n","    # Compile model.\n","    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n","\n","    checkpoint_filepath = \"logs/\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_loss\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[\n","            checkpoint_callback,\n","            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n","        ],\n","    )\n","\n","    return history\n","\n","\n","input_shape = (image_size, image_size, 3)  # input image shape\n","learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 32\n","num_epochs = 100\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","# Size of the transformer layers\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]\n","transformer_layers = 4\n","mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n","\n","\n","history = []\n","num_patches = (image_size // patch_size) ** 2\n","\n","vit_object_detector = create_vit_object_detector(\n","    input_shape,\n","    patch_size,\n","    num_patches,\n","    projection_dim,\n","    num_heads,\n","    transformer_units,\n","    transformer_layers,\n","    mlp_head_units,\n",")\n","\n","# Train model\n","history = run_experiment(\n","    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"TJp1gvT4C8wB","executionInfo":{"status":"error","timestamp":1668880863863,"user_tz":300,"elapsed":116,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"1bedf779-4912-4b38-f774-280a312d1977"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-76ced9c438eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# input image shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'image_size' is not defined"]}]},{"cell_type":"code","source":["import matplotlib.patches as patches\n","\n","# Saves the model in current path\n","vit_object_detector.save(\"vit_object_detector.h5\", save_format=\"h5\")\n","\n","# To calculate IoU (intersection over union, given two bounding boxes)\n","def bounding_box_intersection_over_union(box_predicted, box_truth):\n","    # get (x, y) coordinates of intersection of bounding boxes\n","    top_x_intersect = max(box_predicted[0], box_truth[0])\n","    top_y_intersect = max(box_predicted[1], box_truth[1])\n","    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n","    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n","\n","    # calculate area of the intersection bb (bounding box)\n","    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n","        0, bottom_y_intersect - top_y_intersect + 1\n","    )\n","\n","    # calculate area of the prediction bb and ground-truth bb\n","    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n","        box_predicted[3] - box_predicted[1] + 1\n","    )\n","    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n","        box_truth[3] - box_truth[1] + 1\n","    )\n","\n","    # calculate intersection over union by taking intersection\n","    # area and dividing it by the sum of predicted bb and ground truth\n","    # bb areas subtracted by  the interesection area\n","\n","    # return ioU\n","    return intersection_area / float(\n","        box_predicted_area + box_truth_area - intersection_area\n","    )\n","\n","\n","i, mean_iou = 0, 0\n","\n","# Compare results for 10 images in the test set\n","for input_image in x_test[:10]:\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n","    im = input_image\n","\n","    # Display the image\n","    ax1.imshow(im.astype(\"uint8\"))\n","    ax2.imshow(im.astype(\"uint8\"))\n","\n","    input_image = cv2.resize(\n","        input_image, (image_size, image_size), interpolation=cv2.INTER_AREA\n","    )\n","    input_image = np.expand_dims(input_image, axis=0)\n","    preds = vit_object_detector.predict(input_image)[0]\n","\n","    (h, w) = (im).shape[0:2]\n","\n","    top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)\n","\n","    bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)\n","\n","    box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n","    # Create the bounding box\n","    rect = patches.Rectangle(\n","        (top_left_x, top_left_y),\n","        bottom_right_x - top_left_x,\n","        bottom_right_y - top_left_y,\n","        facecolor=\"none\",\n","        edgecolor=\"red\",\n","        linewidth=1,\n","    )\n","    # Add the bounding box to the image\n","    ax1.add_patch(rect)\n","    ax1.set_xlabel(\n","        \"Predicted: \"\n","        + str(top_left_x)\n","        + \", \"\n","        + str(top_left_y)\n","        + \", \"\n","        + str(bottom_right_x)\n","        + \", \"\n","        + str(bottom_right_y)\n","    )\n","\n","    top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n","\n","    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n","\n","    box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n","\n","    mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)\n","    # Create the bounding box\n","    rect = patches.Rectangle(\n","        (top_left_x, top_left_y),\n","        bottom_right_x - top_left_x,\n","        bottom_right_y - top_left_y,\n","        facecolor=\"none\",\n","        edgecolor=\"red\",\n","        linewidth=1,\n","    )\n","    # Add the bounding box to the image\n","    ax2.add_patch(rect)\n","    ax2.set_xlabel(\n","        \"Target: \"\n","        + str(top_left_x)\n","        + \", \"\n","        + str(top_left_y)\n","        + \", \"\n","        + str(bottom_right_x)\n","        + \", \"\n","        + str(bottom_right_y)\n","        + \"\\n\"\n","        + \"IoU\"\n","        + str(bounding_box_intersection_over_union(box_predicted, box_truth))\n","    )\n","    i = i + 1\n","\n","print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"d0GiCvTXDDdu","executionInfo":{"status":"error","timestamp":1668880867195,"user_tz":300,"elapsed":170,"user":{"displayName":"Sohan Birajdar","userId":"04912918508484739509"}},"outputId":"8e4bd413-0938-498d-be96-31f09cdbd55d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-bbd871f02d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Saves the model in current path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvit_object_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vit_object_detector.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# To calculate IoU (intersection over union, given two bounding boxes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vit_object_detector' is not defined"]}]},{"cell_type":"code","source":["# standard PyTorch mean-std input image normalization\n","transform = T.Compose([\n","    T.Resize(800),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# for output bounding box post-processing\n","def box_cxcywh_to_xyxy(x):\n","    x_c, y_c, w, h = x.unbind(1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","    return torch.stack(b, dim=1)\n","\n","def rescale_bboxes(out_bbox, size):\n","    img_w, img_h = size\n","    b = box_cxcywh_to_xyxy(out_bbox)\n","    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n","    return b"],"metadata":{"id":"aNqDhAEm-59T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CLASSES = ['Elkhorn', 'Staghorn']"],"metadata":{"id":"UzxqnpVr1tXm"},"execution_count":null,"outputs":[]}]}